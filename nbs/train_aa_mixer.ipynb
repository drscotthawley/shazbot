{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp train_aa_mixer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d264b",
   "metadata": {},
   "source": [
    "# train_aa_mixer\n",
    "\n",
    "> Trying to map audio embeddings to vector spaces, for mixing.\n",
    "\n",
    "We try to make the sum of the embeddings of solo parts, equal(/close) to the embedding of the sum (i.e. the full mix).\n",
    "\n",
    "Based on `accelerate`-powered code by Zach Evans & Katherine Crowson, cf. https://github.com/zqevans/audio-diffusion/blob/main/train_diffgan_accel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a29cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ce10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from prefigure.prefigure import get_all_args, push_wandb_config\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import json\n",
    "\n",
    "import accelerate\n",
    "import os, sys\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import optim, nn, Tensor\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as torchdata\n",
    "#from torch.utils import data\n",
    "from tqdm import tqdm, trange\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import wandb\n",
    "import subprocess\n",
    "\n",
    "from shazbot.viz import embeddings_table, pca_point_cloud, audio_spectrogram_image, tokens_spectrogram_image\n",
    "from shazbot.core import n_params, save, freeze, HostPrinter, Mish\n",
    "#import shazbot.blocks_utils as blocks_utils\n",
    "from shazbot.icebox import load_audio_for_jbx, IceBoxModel\n",
    "from shazbot.data import MultiStemDataset\n",
    "\n",
    "\n",
    "# audio-diffusion imports\n",
    "from tqdm import trange\n",
    "import pytorch_lightning as pl\n",
    "from diffusion.pqmf import CachedPQMF as PQMF\n",
    "from diffusion.utils import PadCrop, Stereo, NormInputs\n",
    "from encoders.encoders import RAVEEncoder, ResConvBlock\n",
    "from nwt_pytorch import Memcodes\n",
    "from dvae.residual_memcodes import ResidualMemcodes\n",
    "from decoders.diffusion_decoder import DiffusionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad247898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "#audio diffusion classes\n",
    "class DiffusionDVAE(nn.Module):\n",
    "    def __init__(self, global_args, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.pqmf_bands = global_args.pqmf_bands\n",
    "\n",
    "        if self.pqmf_bands > 1:\n",
    "            self.pqmf = PQMF(2, 70, global_args.pqmf_bands)\n",
    "\n",
    "        self.encoder = RAVEEncoder(2 * global_args.pqmf_bands, 64, global_args.latent_dim, ratios=[2, 2, 2, 2, 4, 4])\n",
    "        self.encoder_ema = deepcopy(self.encoder)\n",
    "            \n",
    "        self.diffusion = DiffusionDecoder(global_args.latent_dim, 2)\n",
    "        self.diffusion_ema = deepcopy(self.diffusion)\n",
    "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "        #self.ema_decay = global_args.ema_decay\n",
    "\n",
    "        self.num_quantizers = global_args.num_quantizers\n",
    "        if self.num_quantizers > 0:\n",
    "            quantizer_class = ResidualMemcodes if global_args.num_quantizers > 1 else Memcodes\n",
    "\n",
    "            quantizer_kwargs = {}\n",
    "            if global_args.num_quantizers > 1:\n",
    "                quantizer_kwargs[\"num_quantizers\"] = global_args.num_quantizers\n",
    "\n",
    "            self.quantizer = quantizer_class(\n",
    "                dim=global_args.latent_dim,\n",
    "                heads=global_args.num_heads,\n",
    "                num_codes=global_args.codebook_size,\n",
    "                temperature=1.,\n",
    "                **quantizer_kwargs\n",
    "            )\n",
    "\n",
    "            self.quantizer_ema = deepcopy(self.quantizer)\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        if self.training:\n",
    "            return self.encoder(*args, **kwargs)\n",
    "        return self.encoder_ema(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        if self.training:\n",
    "            return self.diffusion(*args, **kwargs)\n",
    "        return self.diffusion_ema(*args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam([*self.encoder.parameters(), *self.diffusion.parameters()], lr=2e-5)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        reals = batch[0]\n",
    "\n",
    "        encoder_input = reals\n",
    "\n",
    "        if self.pqmf_bands > 1:\n",
    "            encoder_input = self.pqmf(reals)\n",
    "\n",
    "        # Draw uniformly distributed continuous timesteps\n",
    "        t = self.rng.draw(reals.shape[0])[:, 0].to(self.device)\n",
    "\n",
    "        # Calculate the noise schedule parameters for those timesteps\n",
    "        alphas, sigmas = get_alphas_sigmas(get_crash_schedule(t))\n",
    "\n",
    "        # Combine the ground truth images and the noise\n",
    "        alphas = alphas[:, None, None]\n",
    "        sigmas = sigmas[:, None, None]\n",
    "        noise = torch.randn_like(reals)\n",
    "        noised_reals = reals * alphas + noise * sigmas\n",
    "        targets = noise * alphas - reals * sigmas\n",
    "\n",
    "        # Compute the model output and the loss.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            tokens = self.encoder(encoder_input).float()\n",
    "\n",
    "        if self.num_quantizers > 0:\n",
    "            #Rearrange for Memcodes\n",
    "            tokens = rearrange(tokens, 'b d n -> b n d')\n",
    "\n",
    "            #Quantize into memcodes\n",
    "            tokens, _ = self.quantizer(tokens)\n",
    "\n",
    "            tokens = rearrange(tokens, 'b n d -> b d n')\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            v = self.diffusion(noised_reals, t, tokens)\n",
    "            mse_loss = F.mse_loss(v, targets)\n",
    "            loss = mse_loss\n",
    "\n",
    "        log_dict = {\n",
    "            'train/loss': loss.detach(),\n",
    "            'train/mse_loss': mse_loss.detach(),\n",
    "        }\n",
    "\n",
    "        self.log_dict(log_dict, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "        '''def on_before_zero_grad(self, *args, **kwargs):\n",
    "        decay = 0.95 if self.current_epoch < 25 else self.ema_decay\n",
    "        ema_update(self.diffusion, self.diffusion_ema, decay)\n",
    "        ema_update(self.encoder, self.encoder_ema, decay)\n",
    "\n",
    "        if self.num_quantizers > 0:\n",
    "            ema_update(self.quantizer, self.quantizer_ema, decay)'''\n",
    "\n",
    "        \n",
    "def setup_weights(model, accelerator):\n",
    "    pthfile = 'dvae-checkpoint-june9.pth'\n",
    "    if not os.path.exists(pthfile):\n",
    "        cmd = f'curl -C - -LO https://www.dropbox.com/s/8tcirpokhoxfo82/{pthfile}'\n",
    "        process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "    #self.load_state_dict(torch.load(pthfile))\n",
    "    accelerator.unwrap_model(model).load_state_dict(torch.load(pthfile))\n",
    "    model = model.to(accelerator.device)\n",
    "    return model\n",
    "\n",
    "def ad_encode_it(reals, device, dvaemodel, sample_size=32768, num_quantizers=8):\n",
    "    encoder_input = reals.to(device)\n",
    "    noise = torch.randn([reals.shape[0], 2, sample_size]).to(device)\n",
    "\n",
    "    tokens = dvaemodel.encoder_ema(encoder_input)\n",
    "    if num_quantizers > 0:\n",
    "        #Rearrange for Memcodes\n",
    "        tokens = rearrange(tokens, 'b d n -> b n d')\n",
    "        tokens, _= dvaemodel.quantizer_ema(tokens)\n",
    "        tokens = rearrange(tokens, 'b n d -> b d n')\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d7d92",
   "metadata": {},
   "source": [
    "## The main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class EmbedBlock(nn.Module):\n",
    "    def __init__(self, dims:int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(dims, dims, **kwargs)\n",
    "        #self.act = nn.LeakyReLU()\n",
    "        self.act = Mish()\n",
    "        self.bn = nn.BatchNorm1d(dims)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.lin(x)\n",
    "        x = rearrange(x, 'b d n -> b n d') # gotta rearrange for bn\n",
    "        x = self.bn(x)\n",
    "        x = rearrange(x, 'b n d -> b d n') # and undo rearrange for later layers\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "class AudioAlgebra(nn.Module):\n",
    "    def __init__(self, global_args, device, enc_model):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        #self.encoder = encoder\n",
    "        self.enc_model = enc_model\n",
    "        self.dims = global_args.latent_dim\n",
    "        self.sample_size = global_args.sample_size\n",
    "        self.num_quantizers = global_args.num_quantizers\n",
    "\n",
    "        self.reembedding = nn.Sequential(  # something simple at first\n",
    "            EmbedBlock(self.dims),\n",
    "            EmbedBlock(self.dims),\n",
    "            EmbedBlock(self.dims),\n",
    "            EmbedBlock(self.dims),\n",
    "            EmbedBlock(self.dims),\n",
    "            nn.Linear(self.dims,self.dims)\n",
    "            )\n",
    "\n",
    "    def forward(self,\n",
    "        stems:list,   # list of torch tensors denoting (chunked) solo audio parts to be mixed together\n",
    "        faders:list   # list of gain values to be applied to each stem\n",
    "        ):\n",
    "        \"\"\"We're going to 'on the fly' mix the stems according to the fader settings and generate\n",
    "        frozen-encoder embeddings for each (fader-adjusted) stem and for the total mix.\n",
    "        \"z0\" denotes an embedding from the frozen encoder, \"z\" denotes re-mapped embeddings\n",
    "        in (hopefully) the learned vector space\"\"\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            zs, z0s, zsum, z0sum = [], [], None, None\n",
    "            mix = torch.zeros_like(stems[0]).float()\n",
    "            #print(\"mix.shape = \",mix.shape)\n",
    "            for s, f in zip(stems, faders):\n",
    "                mix_s = s * f             # audio stem adjusted by gain fader f\n",
    "                with torch.no_grad():\n",
    "                    #z0 = self.encoder.encode(mix_s).float()  # initial/frozen embedding/latent for that input\n",
    "                    z0 = ad_encode_it(mix_s, self.device, self.enc_model, sample_size=self.sample_size, num_quantizers=self.num_quantizers)\n",
    "                z0sum = z0 if z0sum is None else z0sum + z0 \n",
    "                #print(\"z0.shape = \",z0.shape)  # most likely [8,32,152]\n",
    "                z0 = rearrange(z0, 'b d n -> b n d')\n",
    "                z = self.reembedding(z0).float()   # <-- this is the main work of the model\n",
    "                zsum = z if zsum is None else zsum + z # compute the sum of all the z's. we'll end up using this in our (metric) loss as \"pred\"\n",
    "                mix += mix_s              # save a record of full audio mix\n",
    "                zs.append(z)              # save a list of individual z's\n",
    "                z0s.append(z0)            # save a list of individual z0's\n",
    "\n",
    "            with torch.no_grad():\n",
    "                #z0mix = self.encoder.encode(mix).float()  # compute frozen embedding / latent for the full mix\n",
    "                z0mix = ad_encode_it(mix, self.device, self.enc_model, sample_size=self.sample_size, num_quantizers=self.num_quantizers)\n",
    "            z0mix = rearrange(z0mix, 'b d n -> b n d')\n",
    "            zmix = self.reembedding(z0mix).float()        # map that according to our learned re-embedding. this will be the \"target\" in the metric loss\n",
    "            z0mix = rearrange(z0mix, 'b n d -> b d n')\n",
    "            \n",
    "            archive = {'zs':zs, 'mix':mix, 'znegsum':None, 'z0s': z0s, 'z0sum':z0sum, 'z0mix':z0mix}\n",
    "\n",
    "        return zsum, zmix, archive    # zsum = pred, zmix = target, and \"archive\" of extra stuff zs & zmix are just for extra info\n",
    "\n",
    "\n",
    "    def mag(self, v):\n",
    "        return torch.norm( v, dim=(1,2) ) # L2 / Frobenius / Euclidean\n",
    "\n",
    "    def distance(self, pred, targ):\n",
    "        return self.mag(pred - targ)\n",
    "    \n",
    "\n",
    "    def loss(self, zsum, zmix, archive, margin=1.0, loss_type='noshrink'):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            dist = self.distance(zsum, zmix) # for each member of batch, compute distance\n",
    "            loss = (dist**2).mean()  # mean across batch; so loss range doesn't change w/ batch_size hyperparam\n",
    "            #print(\"dist = \",dist)\n",
    "            #dist = rearrange(dist, 'b d n -> b (d n)') # flatten non-batch parts\n",
    "            if ('triplet'==loss_type) and (archive['znegsum'] is not None):\n",
    "                negdist = self.distance(archive['znegsum'], zmix)\n",
    "                negdist = negdist * (negdist < margin)   # beyond margin, do nothing\n",
    "                loss = F.relu( (dist**2).mean() - (negdist**2).mean() ) # relu gets us hinge of L2\n",
    "            if ('noshrink' == loss_type):     # try to preserve original magnitudes of of vectors\n",
    "                magdiffs2 = [ ( self.mag(z) - self.mag(z0) )**2 for (z,z0) in zip(archive['zs'], archive['z0s']) ]\n",
    "                loss += 1/300*(sum(magdiffs2)/len(magdiffs2)).mean() # mean of l2 of diff in vector mag  extra .mean() for good measure  \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b64f2",
   "metadata": {},
   "source": [
    "### Reconstruction /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "\n",
    "# Define the noise schedule and sampling loop\n",
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "\n",
    "def get_crash_schedule(t):\n",
    "    sigma = torch.sin(t * math.pi / 2) ** 2\n",
    "    alpha = (1 - sigma ** 2) ** 0.5\n",
    "    return alpha_sigma_to_t(alpha, sigma)\n",
    "\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    \"\"\"Returns a timestep, given the scaling factors for the clean image and for\n",
    "    the noise.\"\"\"\n",
    "    return torch.atan2(sigma, alpha) / math.pi * 2\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, eta, logits, post_every=25):\n",
    "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "\n",
    "    # Create the noise schedule\n",
    "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
    "    alphas, sigmas = get_alphas_sigmas(get_crash_schedule(t))\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in trange(steps):\n",
    "\n",
    "        # Get the model output (v, the predicted velocity)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            v = model(x, ts * t[i], logits).float()\n",
    "\n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        #if 0 == i % post_every: # share intermediate results along the way\n",
    "        #    # can't get the \"plot\" part of \"plot and hear\" to work right\n",
    "        #    display(ipd.Audio(rearrange(pred, 'b d n -> d (b n)').cpu(), rate=44100))\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < steps - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    return pred\n",
    "\n",
    "    \n",
    "def make_eps_model_fn(model):\n",
    "    def eps_model_fn(x, t, **extra_args):\n",
    "        alphas, sigmas = utils.t_to_alpha_sigma(t)\n",
    "        v = model(x, t, **extra_args)\n",
    "        eps = x * sigmas[:, None, None, None] + v * alphas[:, None, None, None]\n",
    "        return eps\n",
    "    return eps_model_fn\n",
    "\n",
    "\n",
    "def make_autocast_model_fn(model, enabled=True):\n",
    "    def autocast_model_fn(*args, **kwargs):\n",
    "        with torch.cuda.amp.autocast(enabled):\n",
    "            return model(*args, **kwargs).float()\n",
    "    return autocast_model_fn\n",
    "\n",
    "\n",
    "def transfer(x, eps, t_1, t_2):\n",
    "    alphas, sigmas = utils.t_to_alpha_sigma(t_1)\n",
    "    next_alphas, next_sigmas = utils.t_to_alpha_sigma(t_2)\n",
    "    pred = (x - eps * sigmas[:, None, None, None]) / alphas[:, None, None, None]\n",
    "    x = pred * next_alphas[:, None, None, None] + eps * next_sigmas[:, None, None, None]\n",
    "    return x, pred\n",
    "\n",
    "\n",
    "def prk_step(model, x, t_1, t_2, extra_args):\n",
    "    eps_model_fn = make_eps_model_fn(model)\n",
    "    t_mid = (t_2 + t_1) / 2\n",
    "    eps_1 = eps_model_fn(x, t_1, **extra_args)\n",
    "    x_1, _ = transfer(x, eps_1, t_1, t_mid)\n",
    "    eps_2 = eps_model_fn(x_1, t_mid, **extra_args)\n",
    "    x_2, _ = transfer(x, eps_2, t_1, t_mid)\n",
    "    eps_3 = eps_model_fn(x_2, t_mid, **extra_args)\n",
    "    x_3, _ = transfer(x, eps_3, t_1, t_2)\n",
    "    eps_4 = eps_model_fn(x_3, t_2, **extra_args)\n",
    "    eps_prime = (eps_1 + 2 * eps_2 + 2 * eps_3 + eps_4) / 6\n",
    "    x_new, pred = transfer(x, eps_prime, t_1, t_2)\n",
    "    return x_new, eps_prime, pred\n",
    "\n",
    "\n",
    "def plms_step(model, x, old_eps, t_1, t_2, extra_args):\n",
    "    eps_model_fn = make_eps_model_fn(model)\n",
    "    eps = eps_model_fn(x, t_1, **extra_args)\n",
    "    eps_prime = (55 * eps - 59 * old_eps[-1] + 37 * old_eps[-2] - 9 * old_eps[-3]) / 24\n",
    "    x_new, _ = transfer(x, eps_prime, t_1, t_2)\n",
    "    _, pred = transfer(x, eps, t_1, t_2)\n",
    "    return x_new, eps, pred\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prk_sample(model, x, steps, extra_args, is_reverse=False, callback=None):\n",
    "    \"\"\"Draws samples from a model given starting noise using fourth-order\n",
    "    Pseudo Runge-Kutta.\"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "    model_fn = make_autocast_model_fn(model)\n",
    "    if not is_reverse:\n",
    "        steps = torch.cat([steps, steps.new_zeros([1])])\n",
    "    for i in trange(len(steps) - 1, disable=None):\n",
    "        x, _, pred = prk_step(model_fn, x, steps[i] * ts, steps[i + 1] * ts, extra_args)\n",
    "        if callback is not None:\n",
    "            callback({'x': x, 'i': i, 't': steps[i], 'pred': pred})\n",
    "    return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plms_sample(model, x, steps, extra_args, is_reverse=False, callback=None):\n",
    "    \"\"\"Draws samples from a model given starting noise using fourth order\n",
    "    Pseudo Linear Multistep.\"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "    model_fn = make_autocast_model_fn(model)\n",
    "    if not is_reverse:\n",
    "        steps = torch.cat([steps, steps.new_zeros([1])])\n",
    "    old_eps = []\n",
    "    for i in trange(len(steps) - 1, disable=None):\n",
    "        if len(old_eps) < 3:\n",
    "            x, eps, pred = prk_step(model_fn, x, steps[i] * ts, steps[i + 1] * ts, extra_args)\n",
    "        else:\n",
    "            x, eps, pred = plms_step(model_fn, x, old_eps, steps[i] * ts, steps[i + 1] * ts, extra_args)\n",
    "            old_eps.pop(0)\n",
    "        old_eps.append(eps)\n",
    "        if callback is not None:\n",
    "            callback({'x': x, 'i': i, 't': steps[i], 'pred': pred})\n",
    "    return x\n",
    "\n",
    "\n",
    "def pie_step(model, x, t_1, t_2, extra_args):\n",
    "    eps_model_fn = make_eps_model_fn(model)\n",
    "    eps_1 = eps_model_fn(x, t_1, **extra_args)\n",
    "    x_1, _ = transfer(x, eps_1, t_1, t_2)\n",
    "    eps_2 = eps_model_fn(x_1, t_2, **extra_args)\n",
    "    eps_prime = (eps_1 + eps_2) / 2\n",
    "    x_new, pred = transfer(x, eps_prime, t_1, t_2)\n",
    "    return x_new, eps_prime, pred\n",
    "\n",
    "\n",
    "def plms2_step(model, x, old_eps, t_1, t_2, extra_args):\n",
    "    eps_model_fn = make_eps_model_fn(model)\n",
    "    eps = eps_model_fn(x, t_1, **extra_args)\n",
    "    eps_prime = (3 * eps - old_eps[-1]) / 2\n",
    "    x_new, _ = transfer(x, eps_prime, t_1, t_2)\n",
    "    _, pred = transfer(x, eps, t_1, t_2)\n",
    "    return x_new, eps, pred\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def pie_sample(model, x, steps, extra_args, is_reverse=False, callback=None):\n",
    "    \"\"\"Draws samples from a model given starting noise using second-order\n",
    "    Pseudo Improved Euler.\"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "    model_fn = make_autocast_model_fn(model)\n",
    "    if not is_reverse:\n",
    "        steps = torch.cat([steps, steps.new_zeros([1])])\n",
    "    for i in trange(len(steps) - 1, disable=None):\n",
    "        x, _, pred = pie_step(model_fn, x, steps[i] * ts, steps[i + 1] * ts, extra_args)\n",
    "        if callback is not None:\n",
    "            callback({'x': x, 'i': i, 't': steps[i], 'pred': pred})\n",
    "    return x\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def plms2_sample(model, x, steps, extra_args, is_reverse=False, callback=None):\n",
    "    \"\"\"Draws samples from a model given starting noise using second order\n",
    "    Pseudo Linear Multistep.\"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "    model_fn = make_autocast_model_fn(model)\n",
    "    if not is_reverse:\n",
    "        steps = torch.cat([steps, steps.new_zeros([1])])\n",
    "    old_eps = []\n",
    "    for i in trange(len(steps) - 1, disable=None):\n",
    "        if len(old_eps) < 1:\n",
    "            x, eps, pred = pie_step(model_fn, x, steps[i] * ts, steps[i + 1] * ts, extra_args)\n",
    "        else:\n",
    "            x, eps, pred = plms2_step(model_fn, x, old_eps, steps[i] * ts, steps[i + 1] * ts, extra_args)\n",
    "            old_eps.pop(0)\n",
    "        old_eps.append(eps)\n",
    "        if callback is not None:\n",
    "            callback({'x': x, 'i': i, 't': steps[i], 'pred': pred})\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def make_cond_model_fn(model, cond):\n",
    "  def cond_model_fn(x, t, **extra_args):\n",
    "    print(x.shape)\n",
    "    print(t.shape)\n",
    "    return model(x, t, cond, **extra_args)\n",
    "  return cond_model_fn\n",
    "\n",
    "\n",
    "def demo(model, log_dict, zsum, zmix, demo_samples, step, demo_steps=250, sr=48000):\n",
    "    demo_batch_size=zsum.shape[0]\n",
    "    \n",
    "    noise = torch.randn([demo_batch_size, 2, demo_samples]).to(model.device)          \n",
    "    model_fn = make_cond_model_fn(model.diffusion_ema, zsum0)\n",
    "\n",
    "    # Run the sampler\n",
    "    fakes = sample(model.diffusion_ema, noise, 500, 1, zsum)\n",
    "    fakes = rearrange(fakes, 'b d n -> d (b n)')\n",
    "    filename = f'zsum_{step:08}.wav'\n",
    "    fakes = fakes.clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "    torchaudio.save(filename, fakes, self.sample_rate)\n",
    "    log_dict['zsum'] = wandb.Audio(filename, sample_rate=sr, caption='zsum')\n",
    "    fakes = sample(model.diffusion_ema, noise, demo_steps, 1, zmix)\n",
    "    fakes = rearrange(fakes, 'b d n -> d (b n)')\n",
    "    filename = f'zmix_{step:08}.wav'\n",
    "    fakes = fakes.clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "    torchaudio.save(filename, fakes, self.sample_rate)\n",
    "    log_dict['zmix'] = wandb.Audio(filename, sample_rate=sr, caption='zmix')                                                          \n",
    "    return log_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8eedc9",
   "metadata": {},
   "source": [
    "### get_stems_faders:\n",
    "really this is more of a `dataloader` utility but for now its being called from the main loop because it involves less change to the dataloader. ;-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_stems_faders(batch, dl, maxstems=6):\n",
    "    \"grab some more audio stems and set faders\"\n",
    "    nstems = 1 + int(torch.randint(maxstems-1,(1,1))[0][0].numpy()) # an int between 1 and maxstems, PyTorch style :-/\n",
    "    faders = 2*torch.rand(nstems)-1  # fader gains can be from -1 to 1\n",
    "    stems = [batch]\n",
    "    dl_iter = iter(dl)\n",
    "    for i in range(nstems-1):\n",
    "        stems.append(next(dl_iter)[0])  # [0] is because there are two items returned and audio is the first\n",
    "    return stems, faders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62783775",
   "metadata": {},
   "source": [
    "## Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40429a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main():\n",
    "\n",
    "    args = get_all_args()\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method(args.start_method)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    accelerator = accelerate.Accelerator()\n",
    "    device = accelerator.device\n",
    "    hprint = HostPrinter(accelerator)\n",
    "    hprint(f'Using device: {device}')\n",
    "\n",
    "    encoder_choices = ['ad','icebox']\n",
    "    encoder_choice = encoder_choices[0]\n",
    "    hprint(f\"Using {encoder_choice} as encoder\")\n",
    "    if 'icebox' == encoder_choice:\n",
    "        args.latent_dim = 64  # overwrite latent_dim with what Jukebox requires\n",
    "        encoder = IceBoxModel(args, device)\n",
    "    elif 'ad' == encoder_choice:\n",
    "        dvae = DiffusionDVAE(args, device)\n",
    "        #dvae = setup_weights(dvae, accelerator, device)\n",
    "        #encoder = dvae.encoder\n",
    "        #freeze(dvae)\n",
    "\n",
    "    hprint(\"Setting up AA model\")\n",
    "    aa_model = AudioAlgebra(args, device, dvae)\n",
    "\n",
    "    hprint(f'  AA Model Parameters: {n_params(aa_model)}')\n",
    "\n",
    "    # If logging to wandb, initialize the run\n",
    "    use_wandb = accelerator.is_main_process and args.name\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        config = vars(args)\n",
    "        config['params'] = n_params(aa_model)\n",
    "        wandb.init(project=args.name, config=config, save_code=True)\n",
    "\n",
    "    opt = optim.Adam([*aa_model.reembedding.parameters()], lr=4e-5)\n",
    "\n",
    "    hprint(\"Setting up dataset\")\n",
    "    train_set = MultiStemDataset([args.training_dir], args)\n",
    "    train_dl = torchdata.DataLoader(train_set, args.batch_size, shuffle=True,\n",
    "                               num_workers=args.num_workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "    hprint(\"Calling accelerator.prepare\")\n",
    "    aa_model, opt, train_dl, dvae = accelerator.prepare(aa_model, opt, train_dl, dvae)\n",
    "\n",
    "    hprint(\"Setting up frozen encoder model weights\")\n",
    "    dvae = setup_weights(dvae, accelerator)\n",
    "    freeze(accelerator.unwrap_model(dvae))\n",
    "    #encoder = dvae.encoder \n",
    "\n",
    "    hprint(\"Setting up wandb\")\n",
    "    if use_wandb:\n",
    "        wandb.watch(aa_model)\n",
    "\n",
    "    hprint(\"Checking for checkpoint\")\n",
    "    if args.ckpt_path:\n",
    "        ckpt = torch.load(args.ckpt_path, map_location='cpu')\n",
    "        accelerator.unwrap_model(aa_model).load_state_dict(ckpt['model'])\n",
    "        opt.load_state_dict(ckpt['opt'])\n",
    "        epoch = ckpt['epoch'] + 1\n",
    "        step = ckpt['step'] + 1\n",
    "        del ckpt\n",
    "    else:\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "\n",
    "    # all set up, let's go\n",
    "    hprint(\"Let's go...\")\n",
    "    try:\n",
    "        while True:  # training loop\n",
    "            #print(f\"Starting epoch {epoch}\")\n",
    "            for batch in tqdm(train_dl, disable=not accelerator.is_main_process):\n",
    "                batch = batch[0]  # first elem is the audio, 2nd is the filename which we don't need\n",
    "                #if accelerator.is_main_process: print(f\"e{epoch} s{step}: got batch. batch.shape = {batch.shape}\")\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # \"batch\" is actually not going to have all the data we want. We could rewrite the dataloader to fix this,\n",
    "                # but instead I just added get_stems_faders() which grabs \"even more\" audio to go with \"batch\"\n",
    "                stems, faders = get_stems_faders(batch, train_dl)\n",
    "\n",
    "                zsum, zmix, zarchive = accelerator.unwrap_model(aa_model).forward(stems,faders)\n",
    "                loss = accelerator.unwrap_model(aa_model).loss(zsum, zmix, zarchive)\n",
    "                accelerator.backward(loss)\n",
    "                opt.step()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if step % 25 == 0:\n",
    "                        tqdm.write(f'Epoch: {epoch}, step: {step}, loss: {loss.item():g}')\n",
    "\n",
    "                    if use_wandb:\n",
    "                        log_dict = {\n",
    "                            'epoch': epoch,\n",
    "                            'loss': loss.item(),\n",
    "                            #'lr': sched.get_last_lr()[0],\n",
    "                            'zsum_pca': pca_point_cloud(zsum.detach()),\n",
    "                            'zmix_pca': pca_point_cloud(zmix.detach())\n",
    "                        }\n",
    "\n",
    "                        if (step % args.demo_every == 0):                                                    \n",
    "                            hprint(\"\\nMaking demo stuff\")\n",
    "\n",
    "                            mix_filename = f'mix_{step:08}.wav'\n",
    "                            reals = zarchive['mix'].clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "                            reals = rearrange(reals, 'b d n -> d (b n)')\n",
    "                            print(\"reals.shape = \",reals.shape)\n",
    "                            torchaudio.save(mix_filename, reals, args.sample_rate)\n",
    "                            log_dict['mix'] = wandb.Audio(mix_filename, sample_rate=args.sample_rate, caption='mix')\n",
    "\n",
    "                            #demo(accelerator.unwrap_model(dvae), log_dict, zsum.detach(), zmix.detach(),  batch.shape[-1], step)\n",
    "                            zsum = zarchive['z0sum'].detach() # rearrange(zarchive['z0sum'], 'b n d -> b d n').detach()\n",
    "                            zmix = zarchive['z0mix'].detach() #rearrange(zarchive['z0mix'], 'b n d -> b d n').detach()\n",
    "\n",
    "                            hprint(f\"zsum.shape = {zsum.shape}\")\n",
    "                            noise = torch.randn([zsum.shape[0], 2, batch.shape[-1]]).to(accelerator.device)\n",
    "                            accelerator.unwrap_model(dvae).diffusion_ema.to(accelerator.device)\n",
    "                            model_fn = make_cond_model_fn(accelerator.unwrap_model(dvae).diffusion_ema, zsum)\n",
    "                            hprint(f\"noise.shape = {noise.shape}\")\n",
    "\n",
    "                            # Run the sampler\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                hprint(\"Calling sampler for zsum\")\n",
    "                                fakes = sample(accelerator.unwrap_model(dvae).diffusion_ema, noise, args.demo_steps, 1, zsum)\n",
    "                            fakes = rearrange(fakes, 'b d n -> d (b n)')\n",
    "                            zsum_filename = f'zsum_{step:08}.wav'\n",
    "                            fakes = fakes.clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "                            torchaudio.save(zsum_filename, fakes, args.sample_rate)\n",
    "                            log_dict['zsum'] = wandb.Audio(zsum_filename, sample_rate=args.sample_rate, caption='zsum')\n",
    "                            \n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                hprint(\"Calling sampler for zmix\")\n",
    "                                fakes = sample(accelerator.unwrap_model(dvae).diffusion_ema, noise, args.demo_steps, 1, zmix)\n",
    "                            fakes = rearrange(fakes, 'b d n -> d (b n)')\n",
    "                            zmix_filename = f'zmix_{step:08}.wav'\n",
    "                            fakes = fakes.clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "                            torchaudio.save(zmix_filename, fakes, args.sample_rate)\n",
    "                            log_dict['zmix'] = wandb.Audio(zmix_filename, sample_rate=args.sample_rate, caption='zmix')\n",
    "                            hprint(\"Done making demo stuff\")\n",
    "                            \n",
    "                    if use_wandb: wandb.log(log_dict, step=step)\n",
    "\n",
    "                if step > 0 and step % args.checkpoint_every == 0:\n",
    "                    save(accelerator, args, aa_model, opt, epoch, step)\n",
    "\n",
    "                step += 1\n",
    "            epoch += 1\n",
    "    except RuntimeError as err:  # ??\n",
    "        import requests\n",
    "        import datetime\n",
    "        ts = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        resp = requests.get('http://169.254.169.254/latest/meta-data/instance-id')\n",
    "        hprint(f'ERROR at {ts} on {resp.text} {device}: {type(err).__name__}: {err}', flush=True)\n",
    "        raise err\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ff373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Not needed if listed in console_scripts in settings.ini\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():  # don't execute in notebook\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61fdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
